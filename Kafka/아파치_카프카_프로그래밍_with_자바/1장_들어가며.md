# 1장 들어가며
## 1.1 카프카의 탄생
데이터를 생성하고 적재하기 위해서는 데이터를 생성하는 소스 애플리케이션과 데이터가 최종 적재되는 타깃 애플리케이션을 연결해야 한다.  
- 초기: 단방향 통신, 아키텍처가 복잡하지 않았으므로 운영이 힘들지 않았다.  
- 시간이 지나면서 아키텍처가 거대해져 문제가 생겼다.  
  
소스 애플리케이션과 타깃 애플리케이션을 연결하는 파이프라인 개수가 많아지면서 소스코드 및 버전 관리에서 이슈가 생겼다. 그리고 타깃 애플리케이션에 장애가 생길 경우 그 영향이 소스 애플리케이션에 그대로 전달되었다.  
  
신규 시스템을 만들기로 결정했고 그 결과물이 바로 아파치 카프카다.  
카프카는 각각의 애플리케이션끼리 연결하여 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙집중화했다. 기존에 1:1 매칭으로 개발하고 운영하던 데이터 파이프라인은 커플링으로 인해 한쪽의 이슈가 다른 한쪽의 애플리케이션에 영향을 미치곤 했지만, 카프카는 이러한 의존도를 타파하였다.  
  
이제 소스 애플리케이션에서 생성되는 데이터는 어느 타깃 애플리케이션으로 보낼 것인지 고민하지 않고 일단 카프카로 넣으면 된다. 카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO(First In First Out) 방식의 큐 자료구조와 유사하다. **큐에 데이터를 보내는 것이 '프로듀서'이고 큐에서 데이터를 가져가는 것이 '컨슈머'다.**    
  
카프카를 통해 전달할 수 있는 데이터 포맷은 사실상 제한이 없다.(ByteArray로 통신하기 때문에)  
## 1.2 빅데이터 파이프라인에서 카프카의 역할
빅데이터를 저장하고 활용하기 위해서는 일단 생성되는 데이터를 모두 모으는 것이 중요한데, 이때 사용되는 개념이 '데이터 레이크(data lake)'다.  
데이터 웨어하우스와 다르게 필터링되거나 패키지화되지 않은 데이터가 저장된다는 점이 특징이다. 즉, 운영되는 서비스로부터 수집 가능한 모든 데이터를 모으는 것이다.  
  
서비스에서 발생하는 데이터를 데이터 레이크로 모으려면 엔드 투 엔드 방식을 생각해 볼 수 있지만, 점점 서비스가 비대해지고 복잡해지면서 파편화되고 복잡도가 올라간다.  
이를 해결하기 위해서 데이터를 추출하고 변경, 적재하는 과정을 묶은 데이터 파이프라인을 구축해야 한다.  
  
카프카가 왜 데이터 파이프라인으로 적합할까?  
**높은 처리량**
카프카는 프로듀서가 브로커로 데이터를 보낼 때와 컨슈머가 브로커로부터 데이터를 받을 때 모두 묶어서 전송한다. 많은 양의 데이터를 송수신할 때 맺어지는 네트워크 비용은 무시할 수 없는 규모가 된다. 동일한 양의 데이터를 보낼 때 네트워크 통신 횟수를 최소한으로 줄인다면 동일 시간 내에 더 많은 데이터를 전송할 수 있다. 많은 양의 데이터를 묶음 단위로 처리하는 배치로 빠르게 처리할 수 있기 때문에 대용량의 실시간 로그데이터를 처리하는 데에 적합하다. 또한, 파티션 단위를 통해 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬 처리할 수 있다. 파티션 개수만큼 컨슈머 개수를 늘려서 동일 시간당 데이터 처리량을 늘리는 것이다.
  
**확장성**  
데이터가 적을 때는 카프카 클러스터의 브로커를 최소한의 개수로 운영하다가 데이터가 많아지면 클러스터의 브로커 개수를 자연스럽게 늘려 스케일 아웃할 수 있다.  
반대로 데이터 개수가 적어지고 추가 서버들이 더는 필요 없어지면 브로커 개수를 줄여 스케일 인할 수 있다. 카프카의 스케일 아웃, 스케일 인 과정은 클러스터의 무중단 운영을 지원한다.  
  
**영속성**
영속성이란 데이터를 생성한 프로그램이 종료되더라도 사라지지 않은 데이터의 특성을 뜻한다.  
카프카는 다른 메시징 플랫폼과 다르게 전송받은 데이터를 메모리에 저장하지 않고 파일 시스템에 저장한다. 파일 시스템에 데이터를 적재하고 사용하는 것은 보편적으로 느리다고 생각하겠지만, 카프카는 운영체제 레벨에서 파일 시스템을 최대한 활용하는 방법을 적용하였다.  
운영체제에서는 파일 I/O 성능 향상을 위해 페이지 캐시(page cache) 영역을 메모리에 따로 생성하여 사용한다. 페이지 캐시 메모리 영역을 사용하여 한번 읽은 파일 내용은 메모리에 저장시켰다가 다시 사용하는 방식이기 때문에 카프카가 파일 시스템에 저장하고 데이터를 저장, 전송하더라도 처리량이 높은 것이다.  
디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발생으로 인해 급작스럽게 종료되더라도 프로세스를 재시작하여 안전하게 데이터를 다시 처리할 수 있다.  
  
**고가용성**  
3개 이상의 서버들로 운영되는 카프카 클러스터는 일부 서버에 장애가 발생하더라도 무중단으로 안전하고 지속적으로 데이터를 처리할 수 있다. 클러스터로 이루어진 카프카는 데이터의 복제(replication)를 통해 고가용성의 특징을 가지게 되었다. 프로듀서로 전송받은 데이터를 여러 브로커 중 1대의 브로커에만 저장하는 것이 아니라 또 다른 브로커에도 저장하는 것이다. 카프카를 안전하게 운영하기 위해 최소 3개 이상의 브로커로 클러스터를 구성할 것을 추천한다.  
  
높은 처리량, 확장성, 영속성, 고가용성 특징을 가진 카프카는 데이터 파이프라인을 안전하고 확정성 높게 운영할 수 있도록 설계되었다.
  
## 1.3 데이터 레이크 아키텍처와 카프카의 미래
데이터 레이크를 구성하는 아키텍처의 종류는 2가지가 있다. 첫 번째는 '람다 아키텍처'이고 두 번째는 '카파 아키텍처'이다. 람다 아키텍처는 레거시 데이터 수집 플랫폼(엔드 투 엔드)을 개선하기 위해 구성한 아키텍처이다.  
이를 해결하기 위해 기존의 배치 데이터를 처리하는 부분 외에 스피드 레이어(speed layer)라고 불리는 실시간 데이터 ETL(extract, transform and load)작업 영역을 정의한 아키텍처를 만들었다.  
  
카파 아키텍처는 람다 아키텍처와 유사하지만 배치 레이어를 제거하고 모든 데이터를 스피드 레이어에 넣어서 처리한다는 점이 다르다.  
람다 아키텍처에서 단점으로 부각되었던 `로직의 파편화, 디버깅, 배포, 운영 분리`에 대한 이슈를 제거하기 위해 배치 레이어를 제거한 카파 아키텍처는 스피드 레이어에서 데이터를 모두 처리할 수 있었으므로 엔지니어들은 더욱 효율적으로 개발과 운영에 임할 수 있게 되었다.  
그런데 카파 아키텍처는 스피듣 레이어에서 모든 데이터를 처리하므로 서비스에서 생성되는 모든 종류의 데이터를 스트림 처리해야 한다.  
  
```text
배치 데이터는 초, 분, 시간, 일 등으로 한정된 기간 단위 데이터를 뜻한다.
스트림 데이터는 한정되지 않은 데이터로 시작 데이터와 끝 데이터가 명확히 정해지지 않은 데이터를 뜻한다.
```
  
배치 데이터의 스트림 프로세스 처리는 모든 데이터를 로그(log: 데이터의 집합)으로 바라보는 것에서 시작하였다. 이 데이터는 지속적으로 추가가 가능하며 각 데이터에는 일정한 번호(or 타임스탬프)가 붙는다. 배치 데이터를 로그로 표현할 때는 **각 시점의 배치 데이터의 변환 기록을 시간 순서대로 기록**함으로써 각 시점의 모든 스냅샷 데이터를 저장하지 않고도 배치 데이터를 표현할 수 있게 되었다.  
  
로그로 배치 데이터와 스트림 데이터를 저장하고 사용하기 위해서는 변환 기록이 일정 기간 동안 삭제되어서는 안 되고 지속적으로 추가되어야 한다.  
그리고 서비스에서 생성된 모든 데이터가 스피드 레이어에 들어오는 것을 감안하면 스피드 레이어를 구성하는 데이터 플랫폼은 SPOF(Single Point Of Failure)가 될 수 있으므로 반드시 내결함성과 장애 허용 특징을 지녀야 했다.  
  
아직은 카프카를 스트리밍 데이터 레이크로 사용하기 위해 개선해야 하는 부분이 있다.  
우선 자주 접근하지 않는 데이터를 굳이 비싼 자원(브로커의 메모리, 디스크)에 유지할 필요가 없다. 카프카 클러스터에서 자주 접근하지 않는 데이터는 오브젝트 스토리지와 같이 저렴하면서도 안전한 저장소에 옮겨 저장하고 자주 사용하는 데이터만 브로커에서 사용하는 구분 작업이 필요하다.  